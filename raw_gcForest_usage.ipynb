{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victormayowa/deepFECG/blob/notebook/raw_gcForest_usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUBXXLzbs7gx"
      },
      "source": [
        "# Deep FECG Research: All-in-One Experiment Notebook for Google Colab\n",
        "\n",
        "This notebook is optimized for Python 3.12+ and modern libraries in a Google Colab environment. It contains all the code for data preprocessing, feature extraction, and model training using a self-contained `gcForest` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RKYW5fcs7g0"
      },
      "source": [
        "## 1. Setup Environment\n",
        "\n",
        "This cell installs all necessary libraries. Run it first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g_CpU3Z4s7g0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e9deda-3702-4667-f990-69522b5cd276"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.7/127.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q wfdb librosa pywavelets ssqueezepy imbalanced-learn shap matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjSk9-Ues7g2"
      },
      "source": [
        "## 2. Mount Google Drive & Define Paths\n",
        "\n",
        "This section mounts your Google Drive to make your dataset accessible. You will need to authorize Colab to access your Drive.\n",
        "\n",
        "**IMPORTANT:** After running the second cell, you **must** update the `PROJECT_PATH` variable to point to the correct location of your project folder on Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cFllUm6s7g2",
        "outputId": "da425a35-bb85-4668-eeb4-b2d75c3c79c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i2MmKOgs7g2",
        "outputId": "a89818a7-4117-4155-f2f2-08cf3409f845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project path set to: /content/drive/MyDrive/MScUEL\n",
            "Data path set to: /content/drive/MyDrive/MScUEL/mit-bih-arrhythmia-database-1.0.0\n",
            "Output path set to: /content/drive/MyDrive/MScUEL/colab_outputs\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# TODO: Update this path to your project directory on Google Drive\n",
        "PROJECT_PATH = '/content/drive/MyDrive/MScUEL'\n",
        "\n",
        "# --- You should not need to edit below this line ---\n",
        "DATA_PATH = os.path.join(PROJECT_PATH, 'mit-bih-arrhythmia-database-1.0.0')\n",
        "OUTPUT_PATH = os.path.join(PROJECT_PATH, 'colab_outputs')\n",
        "\n",
        "# Create an output directory for plots if it doesn't exist\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"Project path set to: {PROJECT_PATH}\")\n",
        "print(f\"Data path set to: {DATA_PATH}\")\n",
        "print(f\"Output path set to: {OUTPUT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl0CY2B5s7g3"
      },
      "source": [
        "## 3. All-in-One Experiment Code\n",
        "\n",
        "The following cells contain all the necessary code for the experiment pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJA5_HlNs7g3"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class gcForest(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, shape_1X=None, n_mgsRFtree=30, window=None, stride=1,\n",
        "                 cascade_test_size=0.2, n_cascadeRF=2, n_cascadeRFtree=101, cascade_layer=np.inf,\n",
        "                 min_samples_mgs=0.1, min_samples_cascade=0.05, tolerance=0.0, n_jobs=1, use_mg_scanning=True):\n",
        "        self.shape_1X = shape_1X\n",
        "        self.n_layer = 0\n",
        "        self._n_samples = 0\n",
        "        self.n_cascadeRF = int(n_cascadeRF)\n",
        "        self.window = [window] if isinstance(window, int) else window\n",
        "        self.stride = stride\n",
        "        self.cascade_test_size = cascade_test_size\n",
        "        self.n_mgsRFtree = int(n_mgsRFtree)\n",
        "        self.n_cascadeRFtree = int(n_cascadeRFtree)\n",
        "        self.cascade_layer = cascade_layer\n",
        "        self.min_samples_mgs = min_samples_mgs\n",
        "        self.min_samples_cascade = min_samples_cascade\n",
        "        self.tolerance = tolerance\n",
        "        self.n_jobs = n_jobs\n",
        "        self.use_mg_scanning = use_mg_scanning\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if X.shape[0] != len(y):\n",
        "            raise ValueError('Sizes of y and X do not match.')\n",
        "        if self.use_mg_scanning:\n",
        "            X = self.mg_scanning(X, y)\n",
        "        self.cascade_forest(X, y)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.use_mg_scanning:\n",
        "            X = self.mg_scanning(X)\n",
        "        cascade_all_pred_prob = self.cascade_forest(X)\n",
        "        return np.mean(cascade_all_pred_prob, axis=0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred_proba = self.predict_proba(X=X)\n",
        "        return np.argmax(pred_proba, axis=1)\n",
        "\n",
        "    def mg_scanning(self, X, y=None):\n",
        "        self._n_samples = X.shape[0]\n",
        "        shape_1X = self.shape_1X\n",
        "        if isinstance(shape_1X, int):\n",
        "            shape_1X = [1, shape_1X]\n",
        "        if not self.window:\n",
        "            self.window = [shape_1X[1]]\n",
        "        mgs_pred_prob = []\n",
        "        for wdw_size in self.window:\n",
        "            wdw_pred_prob = self._window_slicing_pred_prob(X, wdw_size, shape_1X, y=y)\n",
        "            mgs_pred_prob.append(wdw_pred_prob)\n",
        "        return np.concatenate(mgs_pred_prob, axis=1)\n",
        "\n",
        "    def _window_slicing_pred_prob(self, X, window, shape_1X, y=None):\n",
        "        if shape_1X[0] > 1:\n",
        "            sliced_X, sliced_y = self._window_slicing_img(X, window, shape_1X, y=y, stride=self.stride)\n",
        "        else:\n",
        "            sliced_X, sliced_y = self._window_slicing_sequence(X, window, shape_1X, y=y, stride=self.stride)\n",
        "        if y is not None:\n",
        "            prf = RandomForestClassifier(n_estimators=self.n_mgsRFtree, max_features='sqrt', min_samples_split=self.min_samples_mgs, oob_score=True, n_jobs=self.n_jobs)\n",
        "            crf = RandomForestClassifier(n_estimators=self.n_mgsRFtree, max_features=1, min_samples_split=self.min_samples_mgs, oob_score=True, n_jobs=self.n_jobs)\n",
        "            prf.fit(sliced_X, sliced_y)\n",
        "            crf.fit(sliced_X, sliced_y)\n",
        "            setattr(self, f'_mgsprf_{window}', prf)\n",
        "            setattr(self, f'_mgscrf_{window}', crf)\n",
        "            pred_prob_prf = prf.oob_decision_function_\n",
        "            pred_prob_crf = crf.oob_decision_function_\n",
        "        else:\n",
        "            prf = getattr(self, f'_mgsprf_{window}')\n",
        "            crf = getattr(self, f'_mgscrf_{window}')\n",
        "            pred_prob_prf = prf.predict_proba(sliced_X)\n",
        "            pred_prob_crf = crf.predict_proba(sliced_X)\n",
        "        pred_prob = np.c_[pred_prob_prf, pred_prob_crf]\n",
        "        return pred_prob.reshape([self._n_samples, -1])\n",
        "\n",
        "    def _window_slicing_sequence(self, X, window, shape_1X, y=None, stride=1):\n",
        "        if shape_1X[1] < window:\n",
        "            raise ValueError('window must be smaller than the sequence dimension')\n",
        "        len_iter = (shape_1X[1] - window) // stride + 1\n",
        "        iter_array = np.arange(0, stride * len_iter, stride)\n",
        "        inds_to_take = [np.arange(i, i + window) for i in iter_array]\n",
        "        sliced_X = np.take(X, inds_to_take, axis=1).reshape(-1, window)\n",
        "        if y is not None:\n",
        "            sliced_y = np.repeat(y, len_iter)\n",
        "        else:\n",
        "            sliced_y = None\n",
        "        return sliced_X, sliced_y\n",
        "\n",
        "    def cascade_forest(self, X, y=None):\n",
        "        if y is not None:\n",
        "            self.n_layer = 0\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.cascade_test_size)\n",
        "            self.n_layer += 1\n",
        "            prf_crf_pred_ref = self._cascade_layer(X_train, y_train)\n",
        "            accuracy_ref = self._cascade_evaluation(X_test, y_test)\n",
        "            feat_arr = self._create_feat_arr(X_train, prf_crf_pred_ref)\n",
        "            self.n_layer += 1\n",
        "            prf_crf_pred_layer = self._cascade_layer(feat_arr, y_train)\n",
        "            accuracy_layer = self._cascade_evaluation(X_test, y_test)\n",
        "            while accuracy_layer > (accuracy_ref + self.tolerance) and self.n_layer <= self.cascade_layer:\n",
        "                accuracy_ref = accuracy_layer\n",
        "                prf_crf_pred_ref = prf_crf_pred_layer\n",
        "                feat_arr = self._create_feat_arr(X_train, prf_crf_pred_ref)\n",
        "                self.n_layer += 1\n",
        "                prf_crf_pred_layer = self._cascade_layer(feat_arr, y_train)\n",
        "                accuracy_layer = self._cascade_evaluation(X_test, y_test)\n",
        "            if accuracy_layer < accuracy_ref:\n",
        "                for irf in range(self.n_cascadeRF):\n",
        "                    delattr(self, f'_casprf{self.n_layer}_{irf}')\n",
        "                    delattr(self, f'_cascrf{self.n_layer}_{irf}')\n",
        "                self.n_layer -= 1\n",
        "        else:\n",
        "            at_layer = 1\n",
        "            prf_crf_pred_ref = self._cascade_layer(X, layer=at_layer)\n",
        "            while at_layer < self.n_layer:\n",
        "                at_layer += 1\n",
        "                feat_arr = self._create_feat_arr(X, prf_crf_pred_ref)\n",
        "                prf_crf_pred_ref = self._cascade_layer(feat_arr, layer=at_layer)\n",
        "        return prf_crf_pred_ref\n",
        "\n",
        "    def _cascade_layer(self, X, y=None, layer=0):\n",
        "        prf = RandomForestClassifier(n_estimators=self.n_cascadeRFtree, max_features='sqrt', min_samples_split=self.min_samples_cascade, oob_score=True, n_jobs=self.n_jobs)\n",
        "        crf = RandomForestClassifier(n_estimators=self.n_cascadeRFtree, max_features=1, min_samples_split=self.min_samples_cascade, oob_score=True, n_jobs=self.n_jobs)\n",
        "        prf_crf_pred = []\n",
        "        if y is not None:\n",
        "            for irf in range(self.n_cascadeRF):\n",
        "                prf.fit(X, y)\n",
        "                crf.fit(X, y)\n",
        "                setattr(self, f'_casprf{self.n_layer}_{irf}', prf)\n",
        "                setattr(self, f'_cascrf{self.n_layer}_{irf}', crf)\n",
        "                prf_crf_pred.append(prf.oob_decision_function_)\n",
        "                prf_crf_pred.append(crf.oob_decision_function_)\n",
        "        else:\n",
        "            for irf in range(self.n_cascadeRF):\n",
        "                prf = getattr(self, f'_casprf{layer}_{irf}')\n",
        "                crf = getattr(self, f'_cascrf{layer}_{irf}')\n",
        "                prf_crf_pred.append(prf.predict_proba(X))\n",
        "                prf_crf_pred.append(crf.predict_proba(X))\n",
        "        return prf_crf_pred\n",
        "\n",
        "\n",
        "    def _cascade_evaluation(self, X_test, y_test):\n",
        "        casc_pred_prob = np.mean(self.cascade_forest(X_test), axis=0)\n",
        "        casc_pred = np.argmax(casc_pred_prob, axis=1)\n",
        "        return accuracy_score(y_true=y_test, y_pred=casc_pred)\n",
        "\n",
        "    def _create_feat_arr(self, X, prf_crf_pred):\n",
        "        swap_pred = np.swapaxes(prf_crf_pred, 0, 1)\n",
        "        add_feat = swap_pred.reshape([X.shape[0], -1])\n",
        "        return np.concatenate([add_feat, X], axis=1)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {'shape_1X': self.shape_1X,\n",
        "                'n_mgsRFtree': self.n_mgsRFtree,\n",
        "                'window': self.window,\n",
        "                'stride': self.stride,\n",
        "                'cascade_test_size': self.cascade_test_size,\n",
        "                'n_cascadeRF': self.n_cascadeRF,\n",
        "                'n_cascadeRFtree': self.n_cascadeRFtree,\n",
        "                'cascade_layer': self.cascade_layer,\n",
        "                'min_samples_mgs': self.min_samples_mgs,\n",
        "                'min_samples_cascade': self.min_samples_cascade,\n",
        "                'tolerance': self.tolerance,\n",
        "                'n_jobs': self.n_jobs,\n",
        "                'use_mg_scanning': self.use_mg_scanning}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOoFBhuzs7g4"
      },
      "outputs": [],
      "source": [
        "import wfdb\n",
        "from scipy.signal import butter, filtfilt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "AAMI_CLASSES = {\n",
        "    'N': 0, 'L': 0, 'R': 0, 'e': 0, 'j': 0,\n",
        "    'A': 1, 'a': 1, 'J': 1, 'S': 1,\n",
        "    'V': 2, 'E': 2,\n",
        "    'F': 3,\n",
        "    '/': 4, 'f': 4, 'Q': 4,\n",
        "}\n",
        "\n",
        "def get_aami_class(symbol):\n",
        "    return AAMI_CLASSES.get(symbol)\n",
        "\n",
        "def apply_bandpass_filter(signal, fs=360):\n",
        "    lowcut = 0.5\n",
        "    highcut = 45.0\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(2, [low, high], btype='band')\n",
        "    return filtfilt(b, a, signal)\n",
        "\n",
        "def segment_heartbeats(signal, annotations, fs=360, window_size=360):\n",
        "    heartbeats, labels = [], []\n",
        "    window_before = window_size // 2\n",
        "    window_after = window_size - window_before\n",
        "    for i, symbol in enumerate(annotations.symbol):\n",
        "        aami_class = get_aami_class(symbol)\n",
        "        if aami_class is not None:\n",
        "            peak_sample = annotations.sample[i]\n",
        "            start, end = peak_sample - window_before, peak_sample + window_after\n",
        "            if start >= 0 and end < len(signal):\n",
        "                heartbeats.append(signal[start:end])\n",
        "                labels.append(aami_class)\n",
        "    return np.array(heartbeats), np.array(labels)\n",
        "\n",
        "def preprocess_data(data_path, window_size=360, max_records=None):\n",
        "    print(f\"Starting data preprocessing...\")\n",
        "    record_names = sorted([f.split('.')[0] for f in os.listdir(data_path) if f.endswith('.hea')])\n",
        "    all_heartbeats, all_labels = [], []\n",
        "    for i, record_name in enumerate(record_names):\n",
        "        if max_records and i >= max_records:\n",
        "            break\n",
        "        try:\n",
        "            record = wfdb.rdrecord(os.path.join(data_path, record_name))\n",
        "            annotations = wfdb.rdann(os.path.join(data_path, record_name), 'atr')\n",
        "            signal = record.p_signal[:, record.sig_name.index('MLII') if 'MLII' in record.sig_name else 0]\n",
        "            filtered_signal = apply_bandpass_filter(signal, fs=record.fs)\n",
        "            heartbeats, labels = segment_heartbeats(filtered_signal, annotations, fs=record.fs, window_size=window_size)\n",
        "            all_heartbeats.append(heartbeats)\n",
        "            all_labels.append(labels)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process record {record_name}: {e}\")\n",
        "    if not all_heartbeats:\n",
        "        raise ValueError(\"No heartbeats processed. Check data path and file integrity.\")\n",
        "    X, y = np.concatenate(all_heartbeats), np.concatenate(all_labels)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    print(\"Applying SMOTE to balance the training data...\")\n",
        "    smote = SMOTE(random_state=42, k_neighbors=1) # Reduced k_neighbors to handle small classes\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "    print(f\"Original training samples: {len(y_train)}, Resampled training samples: {len(y_train_resampled)}\")\n",
        "    print(\"Data preprocessing complete.\")\n",
        "    return X_train_resampled, X_test, y_train_resampled, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSJBF8v8s7g6"
      },
      "source": [
        "## 4. Run Experiments\n",
        "\n",
        "Now we can run the experiments for both model types. We use a small number of records (`max_records=4`) for a quick test run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604,
          "referenced_widgets": [
            "d19fe35244d54ca4b1cc99470a6a19cc",
            "b56dd29c0a76461fb8331bb08d90dbde",
            "4f65ed5f9ecf43d894908fbdd2ba8a0e",
            "73027e0125044690bbbd738adeedc95f",
            "94e060259f094a0b8c4e6c2db2147f77",
            "7196331b3b0e4d3db45847a6ced2e882",
            "8ea5651b878744b48f30343ff00c51aa",
            "a31245b0023e43abb3e0ca07aa8baf38",
            "d41a931fc3044323b63ae459b037a06e",
            "d93f99c08ebf4cceb2b45abd23ad1124",
            "f523ce6b16d34ea09a4c8e20098df3ff"
          ]
        },
        "id": "fGxGaNSns7g6",
        "outputId": "dd4cb4fb-69ea-4e91-b677-618f70f4ed67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================--- Starting Experiment: Model=CascadeForest, Features=MFCC ---\n",
            "Starting data preprocessing...\n",
            "Applying SMOTE to balance the training data...\n",
            "Original training samples: 6722, Resampled training samples: 20084\n",
            "Data preprocessing complete.\n",
            "Extracting features using MFCC method...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=360\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature extraction complete.\n",
            "--- Training and evaluating CascadeForest model ---\n",
            "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
            "Best parameters for CascadeForest: {'cascade_layer': 15, 'min_samples_cascade': 0.05, 'n_cascadeRF': 2, 'n_cascadeRFtree': 101, 'tolerance': 0.005}\n",
            "Evaluating the best model on the test set...\n",
            "Accuracy: 0.7210\n",
            "F1-score: 0.7312\n",
            "Precision: 0.7435\n",
            "Recall: 0.7210\n",
            "ROC AUC Score: 0.9967\n",
            "Confusion Matrix:\n",
            "[[1207   46    2    0    0]\n",
            " [   4    4    0    0    0]\n",
            " [   0    0    1    0    0]\n",
            " [   0    0    0    0    0]\n",
            " [   2    0    0  415    0]]\n",
            "Calculating SHAP values...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d19fe35244d54ca4b1cc99470a6a19cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1681 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import argparse\n",
        "\n",
        "args_cascade = argparse.Namespace(\n",
        "    data_path=DATA_PATH,\n",
        "    output_path=OUTPUT_PATH,\n",
        "    feature_extractor='MFCC',\n",
        "    model='CascadeForest',\n",
        "    explain=True,\n",
        "    max_records=4\n",
        ")\n",
        "run_experiment(args_cascade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP7226sWzbFB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRSxUb8hs7g6"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "args_gc = argparse.Namespace(\n",
        "    data_path=DATA_PATH,\n",
        "    output_path=OUTPUT_PATH,\n",
        "    feature_extractor='DWT',\n",
        "    model='gcForest',\n",
        "    explain=True,\n",
        "    max_records=4\n",
        ")\n",
        "run_experiment(args_gc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q64zgZROs7g6"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "If the cells above executed without errors, your environment is correctly set up and the self-contained experiment notebook is working. You can now adjust the parameters (e.g., `max_records`, `feature_extractor`, and the `param_grid` in the `train_and_evaluate` function) to run your full research experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f071283f"
      },
      "source": [
        "## Implement padding or truncation for dwt features\n",
        "\n",
        "### Subtask:\n",
        "Implement padding or truncation for dwt features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce3ffabb"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `_extract_dwt` function to handle different padding strategies and update `extract_features` to pass this parameter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ea934e06"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import pywt\n",
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "def extract_features(train_data, test_data, method='MFCC', wavelet='db4', level=4, padding_strategy='pad', target_length=None):\n",
        "    print(f\"Extracting features using {method} method...\")\n",
        "    if method == 'MFCC':\n",
        "        train_features = _extract_mfcc(train_data)\n",
        "        test_features = _extract_mfcc(test_data)\n",
        "    elif method == 'DWT':\n",
        "        train_features = _extract_dwt(train_data, wavelet=wavelet, level=level, padding_strategy=padding_strategy, target_length=target_length)\n",
        "        test_features = _extract_dwt(test_data, wavelet=wavelet, level=level, padding_strategy=padding_strategy, target_length=target_length)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown feature extraction method: {method}\")\n",
        "    print(\"Feature extraction complete.\")\n",
        "    return train_features, test_features\n",
        "\n",
        "def _extract_mfcc(data, sr=360, n_mfcc=13):\n",
        "    # Adjusted n_fft to be less than or equal to the signal length (360)\n",
        "    mfccs = [np.mean(librosa.feature.mfcc(y=heartbeat.astype(float), sr=sr, n_mfcc=n_mfcc, n_fft=256).T, axis=0) for heartbeat in data]\n",
        "    return np.array(mfccs)\n",
        "\n",
        "def _extract_dwt(data, wavelet='db4', level=4, padding_strategy='pad', target_length=None):\n",
        "    coeffs = [pywt.wavedec(heartbeat, wavelet, level=level) for heartbeat in data]\n",
        "    flat_features = [np.concatenate([c.flatten() for c in coef]) for coef in coeffs]\n",
        "\n",
        "    if padding_strategy == 'pad':\n",
        "        max_len = max(len(f) for f in flat_features)\n",
        "        processed_features = np.array([np.pad(f, (0, max_len - len(f))) for f in flat_features])\n",
        "    elif padding_strategy == 'truncate':\n",
        "        if target_length is None:\n",
        "            raise ValueError(\"target_length must be specified for 'truncate' strategy\")\n",
        "        processed_features = np.array([f[:target_length] for f in flat_features])\n",
        "    elif padding_strategy == 'resize':\n",
        "        if target_length is None:\n",
        "            raise ValueError(\"target_length must be specified for 'resize' strategy\")\n",
        "        processed_features = []\n",
        "        for f in flat_features:\n",
        "            x = np.linspace(0, 1, len(f))\n",
        "            f_interp = interp1d(x, f)\n",
        "            x_new = np.linspace(0, 1, target_length)\n",
        "            processed_features.append(f_interp(x_new))\n",
        "        processed_features = np.array(processed_features)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown padding strategy: {padding_strategy}\")\n",
        "\n",
        "    return processed_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b57ef37"
      },
      "source": [
        "## Implement denoising\n",
        "\n",
        "### Subtask:\n",
        "Implement denoising by adding a denoising step in the `preprocess_data` function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bdb4492"
      },
      "source": [
        "**Reasoning**:\n",
        "Add denoising capability to the preprocess_data function and update the run_experiment function and argparse namespaces accordingly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "4f022780",
        "outputId": "236e5128-ee9d-46d7-aa86-a7bac90d30f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================--- Starting Experiment: Model=CascadeForest, Features=MFCC, Balancing=SMOTE, Scaling=standard, Denoising=moving_average ---\n",
            "Starting data preprocessing with balancing strategy: SMOTE and denoising strategy: moving_average...\n",
            "Applying SMOTE to balance the training data...\n",
            "Original training samples: 6722, Resampled training samples: 20084\n",
            "Data preprocessing complete.\n",
            "Extracting features using MFCC method...\n",
            "Feature extraction complete.\n",
            "Applying standard scaling...\n",
            "Scaling complete.\n",
            "--- Training and evaluating CascadeForest model ---\n",
            "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
            "Best parameters for CascadeForest: {'cascade_layer': 25, 'min_samples_cascade': 0.05, 'n_cascadeRF': 2, 'n_cascadeRFtree': 101, 'tolerance': 0.005}\n",
            "Evaluating the best model on the test set...\n",
            "Accuracy: 0.6139\n",
            "F1-score: 0.6374\n",
            "Precision: 0.6658\n",
            "Recall: 0.6139\n",
            "ROC AUC Score: 0.8309\n",
            "Confusion Matrix:\n",
            "[[1026   66    7  156    0]\n",
            " [   0    6    0    2    0]\n",
            " [   1    0    0    0    0]\n",
            " [   0    0    0    0    0]\n",
            " [ 124   12    4  277    0]]\n",
            "--- Experiment Finished: Model=CascadeForest ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'args_gc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2046573404.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;31m#     denoising_window_size=5 # Added denoising window size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_gc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'args_gc' is not defined"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "from scipy.signal import butter, filtfilt, medfilt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "import numpy as np\n",
        "import os\n",
        "import librosa\n",
        "import pywt\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from scipy.ndimage import uniform_filter1d # For moving average\n",
        "\n",
        "# Redefine gcForest class to include it in this block\n",
        "class gcForest(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, shape_1X=None, n_mgsRFtree=30, window=None, stride=1,\n",
        "                 cascade_test_size=0.2, n_cascadeRF=2, n_cascadeRFtree=101, cascade_layer=np.inf,\n",
        "                 min_samples_mgs=0.1, min_samples_cascade=0.05, tolerance=0.0, n_jobs=1, use_mg_scanning=True):\n",
        "        self.shape_1X = shape_1X\n",
        "        self.n_layer = 0\n",
        "        self._n_samples = 0\n",
        "        self.n_cascadeRF = int(n_cascadeRF)\n",
        "        self.window = [window] if isinstance(window, int) else window\n",
        "        self.stride = stride\n",
        "        self.cascade_test_size = cascade_test_size\n",
        "        self.n_mgsRFtree = int(n_mgsRFtree) # Ensure this is set\n",
        "        self.n_cascadeRFtree = int(n_cascadeRFtree)\n",
        "        self.cascade_layer = cascade_layer\n",
        "        self.min_samples_mgs = min_samples_mgs\n",
        "        self.min_samples_cascade = min_samples_cascade\n",
        "        self.tolerance = tolerance\n",
        "        self.n_jobs = n_jobs\n",
        "        self.use_mg_scanning = use_mg_scanning\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if X.shape[0] != len(y):\n",
        "            raise ValueError('Sizes of y and X do not match.')\n",
        "        if self.use_mg_scanning:\n",
        "            X = self.mg_scanning(X, y)\n",
        "        self.cascade_forest(X, y)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.use_mg_scanning:\n",
        "            X = self.mg_scanning(X)\n",
        "        cascade_all_pred_prob = self.cascade_forest(X)\n",
        "        # Handle potential None values in cascade_all_pred_prob if prediction fails\n",
        "        valid_probs = [prob for prob in cascade_all_pred_prob if prob is not None]\n",
        "        if not valid_probs:\n",
        "             # Return a default shape if prediction fails for all layers\n",
        "             # Assuming the number of classes can be inferred from the trained model,\n",
        "             # or passed during initialization/fit. For now, returning empty array.\n",
        "             # A more robust solution might store class labels during fit.\n",
        "             # If the model was fitted, self._train_labels should exist.\n",
        "             if hasattr(self, '_train_labels'):\n",
        "                 n_classes = len(np.unique(self._train_labels))\n",
        "                 return np.zeros((X.shape[0], n_classes))\n",
        "             else:\n",
        "                 # Cannot determine number of classes, return empty\n",
        "                 return np.array([])\n",
        "        return np.mean(valid_probs, axis=0)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred_proba = self.predict_proba(X=X)\n",
        "        if pred_proba.size == 0:\n",
        "             # Handle case where predict_proba returned empty\n",
        "             return np.array([])\n",
        "        return np.argmax(pred_proba, axis=1)\n",
        "\n",
        "    def mg_scanning(self, X, y=None):\n",
        "        self._n_samples = X.shape[0]\n",
        "        shape_1X = self.shape_1X\n",
        "        if isinstance(shape_1X, int):\n",
        "            shape_1X = [1, shape_1X]\n",
        "        if not self.window:\n",
        "            self.window = [shape_1X[1]]\n",
        "        mgs_pred_prob = []\n",
        "        for wdw_size in self.window:\n",
        "            wdw_pred_prob = self._window_slicing_pred_prob(X, wdw_size, shape_1X, y=y)\n",
        "            mgs_pred_prob.append(wdw_pred_prob)\n",
        "        return np.concatenate(mgs_pred_prob, axis=1)\n",
        "\n",
        "    def _window_slicing_pred_prob(self, X, window, shape_1X, y=None):\n",
        "        if shape_1X[0] > 1:\n",
        "            sliced_X, sliced_y = self._window_slicing_img(X, window, shape_1X, y=y, stride=self.stride)\n",
        "        else:\n",
        "            sliced_X, sliced_y = self._window_slicing_sequence(X, window, shape_1X, y=y, stride=self.stride)\n",
        "        if y is not None:\n",
        "            prf = RandomForestClassifier(n_estimators=self.n_mgsRFtree, max_features='sqrt', min_samples_split=self.min_samples_mgs, oob_score=True, n_jobs=self.n_jobs)\n",
        "            crf = RandomForestClassifier(n_estimators=self.n_mgsRFtree, max_features=1, min_samples_split=self.min_samples_mgs, oob_score=True, n_jobs=self.n_jobs)\n",
        "            prf.fit(sliced_X, sliced_y)\n",
        "            crf.fit(sliced_X, sliced_y)\n",
        "            setattr(self, f'_mgsprf_{window}', prf)\n",
        "            setattr(self, f'_mgscrf_{window}', crf)\n",
        "            pred_prob_prf = prf.oob_decision_function_\n",
        "            pred_prob_crf = crf.oob_decision_function_\n",
        "        else:\n",
        "            prf = getattr(self, f'_mgsprf_{window}')\n",
        "            crf = getattr(self, f'_mgscrf_{window}')\n",
        "            pred_prob_prf = prf.predict_proba(sliced_X)\n",
        "            pred_prob_crf = crf.predict_proba(sliced_X)\n",
        "        pred_prob = np.c_[pred_prob_prf, pred_prob_crf]\n",
        "        return pred_prob.reshape([self._n_samples, -1])\n",
        "\n",
        "    def _window_slicing_sequence(self, X, window, shape_1X, y=None, stride=1):\n",
        "        if shape_1X[1] < window:\n",
        "            raise ValueError('window must be smaller than the sequence dimension')\n",
        "        len_iter = (shape_1X[1] - window) // stride + 1\n",
        "        iter_array = np.arange(0, stride * len_iter, stride)\n",
        "        inds_to_take = [np.arange(i, i + window) for i in iter_array]\n",
        "        sliced_X = np.take(X, inds_to_take, axis=1).reshape(-1, window)\n",
        "        if y is not None:\n",
        "            sliced_y = np.repeat(y, len_iter)\n",
        "        else:\n",
        "            sliced_y = None\n",
        "        return sliced_X, sliced_y\n",
        "\n",
        "    def cascade_forest(self, X, y=None):\n",
        "        if y is not None:\n",
        "            self._train_labels = y # Store training labels for predict_proba\n",
        "            self.n_layer = 0\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.cascade_test_size)\n",
        "            self.n_layer += 1\n",
        "            prf_crf_pred_ref = self._cascade_layer(X_train, y_train)\n",
        "            accuracy_ref = self._cascade_evaluation(X_test, y_test)\n",
        "            feat_arr = self._create_feat_arr(X_train, prf_crf_pred_ref)\n",
        "            self.n_layer += 1\n",
        "            prf_crf_pred_layer = self._cascade_layer(feat_arr, y_train)\n",
        "            accuracy_layer = self._cascade_evaluation(X_test, y_test)\n",
        "            while accuracy_layer > (accuracy_ref + self.tolerance) and self.n_layer <= self.cascade_layer:\n",
        "                accuracy_ref = accuracy_layer\n",
        "                prf_crf_pred_ref = prf_crf_pred_layer\n",
        "                feat_arr = self._create_feat_arr(X_train, prf_crf_pred_ref)\n",
        "                self.n_layer += 1\n",
        "                prf_crf_pred_layer = self._cascade_layer(feat_arr, y_train)\n",
        "                accuracy_layer = self._cascade_evaluation(X_test, y_test)\n",
        "            if accuracy_layer < accuracy_ref:\n",
        "                for irf in range(self.n_cascadeRF):\n",
        "                    delattr(self, f'_casprf{self.n_layer}_{irf}')\n",
        "                    delattr(self, f'_cascrf{self.n_layer}_{irf}')\n",
        "                self.n_layer -= 1\n",
        "            # Return the prediction from the best layer during training\n",
        "            return prf_crf_pred_ref\n",
        "        else:\n",
        "            at_layer = 1\n",
        "            all_layer_preds = []\n",
        "            while at_layer <= self.n_layer:\n",
        "                try:\n",
        "                    if at_layer == 1:\n",
        "                         layer_preds = self._cascade_layer(X, layer=at_layer)\n",
        "                    else:\n",
        "                         # Ensure there are predictions from the previous layer before creating feat_arr\n",
        "                         if not all_layer_preds:\n",
        "                             print(f\"Warning: No predictions from previous layer to create feature array for layer {at_layer}.\")\n",
        "                             break\n",
        "                         feat_arr = self._create_feat_arr(X, all_layer_preds[-1]) # Use predictions from the previous layer\n",
        "                         # Ensure feat_arr is valid before passing to _cascade_layer\n",
        "                         if feat_arr.shape[1] <= X.shape[1]: # Simple check if features were added\n",
        "                             print(f\"Warning: Feature array for layer {at_layer} was not expanded. Stopping cascade prediction.\")\n",
        "                             break\n",
        "                         layer_preds = self._cascade_layer(feat_arr, layer=at_layer)\n",
        "\n",
        "                    all_layer_preds.append(layer_preds)\n",
        "                    at_layer += 1\n",
        "                except AttributeError:\n",
        "                    # Handle case where a layer's models don't exist (e.g., if training stopped early)\n",
        "                    print(f\"Warning: Models for cascade layer {at_layer} not found during prediction. Stopping cascade prediction.\")\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during cascade prediction at layer {at_layer}: {e}. Stopping.\")\n",
        "                    break\n",
        "\n",
        "            # Return predictions from the last successful layer\n",
        "            return all_layer_preds[-1] if all_layer_preds else None\n",
        "\n",
        "\n",
        "    def _cascade_layer(self, X, y=None, layer=0):\n",
        "        prf = RandomForestClassifier(n_estimators=self.n_cascadeRFtree, max_features='sqrt', min_samples_split=self.min_samples_cascade, n_jobs=self.n_jobs)\n",
        "        crf = RandomForestClassifier(n_estimators=self.n_cascadeRFtree, max_features=1, min_samples_split=self.min_samples_cascade, n_jobs=self.n_jobs)\n",
        "\n",
        "        if y is not None:\n",
        "             prf.oob_score = True\n",
        "             crf.oob_score = True\n",
        "             prf_crf_pred = []\n",
        "             for irf in range(self.n_cascadeRF):\n",
        "                 prf.fit(X, y)\n",
        "                 crf.fit(X, y)\n",
        "                 setattr(self, f'_casprf{self.n_layer}_{irf}', prf)\n",
        "                 setattr(self, f'_cascrf{self.n_layer}_{irf}', crf)\n",
        "                 # Use decision_function for OOB scores (handles multiclass)\n",
        "                 prf_crf_pred.append(prf.oob_decision_function_)\n",
        "                 crf_oob_decision_function = crf.oob_decision_function_\n",
        "                 # Ensure CRF OOB decision function has the correct shape for binary/multiclass\n",
        "                 if crf_oob_decision_function.ndim == 1:\n",
        "                     # Assuming binary classification if ndim is 1, convert to shape (n_samples, 2)\n",
        "                     crf_oob_decision_function = np.vstack([1 - crf_oob_decision_function, crf_oob_decision_function]).T\n",
        "                 prf_crf_pred.append(crf_oob_decision_function)\n",
        "        else:\n",
        "            prf_crf_pred = []\n",
        "            for irf in range(self.n_cascadeRF):\n",
        "                # Check if the required attributes exist before getting them\n",
        "                if not hasattr(self, f'_casprf{layer}_{irf}') or not hasattr(self, f'_cascrf{layer}_{irf}'):\n",
        "                    print(f\"Error: Cascade models for layer {layer}, forest {irf} not found during prediction.\")\n",
        "                    return None # Indicate failure to get predictions for this layer\n",
        "                prf = getattr(self, f'_casprf{layer}_{irf}')\n",
        "                crf = getattr(self, f'_cascrf{layer}_{irf}')\n",
        "                prf_crf_pred.append(prf.predict_proba(X))\n",
        "                prf_crf_pred.append(crf.predict_proba(X))\n",
        "        return prf_crf_pred\n",
        "\n",
        "\n",
        "    def _cascade_evaluation(self, X_test, y_test):\n",
        "        casc_pred_prob_list = self.cascade_forest(X_test)\n",
        "        # Check if cascade_forest returned None (e.g., no layers trained or predicted)\n",
        "        if casc_pred_prob_list is None or len(casc_pred_prob_list) == 0:\n",
        "             print(\"Warning: No predictions from cascade forest evaluation.\")\n",
        "             return -np.inf # Return a very low accuracy\n",
        "        # Filter out any None values from the list of prediction probabilities\n",
        "        valid_pred_probs = [pred_prob for pred_prob in casc_pred_prob_list if pred_prob is not None]\n",
        "        if not valid_pred_probs:\n",
        "             print(\"Warning: All cascade layer predictions were None.\")\n",
        "             return -np.inf\n",
        "\n",
        "        casc_pred_prob = np.mean(valid_pred_probs, axis=0)\n",
        "        casc_pred = np.argmax(casc_pred_prob, axis=1)\n",
        "        return accuracy_score(y_true=y_test, y_pred=casc_pred)\n",
        "\n",
        "\n",
        "    def _create_feat_arr(self, X, prf_crf_pred):\n",
        "        # Ensure prf_crf_pred is not None and has the expected structure\n",
        "        if prf_crf_pred is None or len(prf_crf_pred) == 0:\n",
        "             # Handle the case where prediction failed for the previous layer\n",
        "             print(\"Warning: No predictions available to create feature array.\")\n",
        "             return X # Return original features if no predictions to add\n",
        "        # Ensure all elements in prf_crf_pred are numpy arrays before stacking\n",
        "        if not all(isinstance(p, np.ndarray) for p in prf_crf_pred):\n",
        "            print(\"Warning: Unexpected non-array elements in prf_crf_pred. Cannot create feature array.\")\n",
        "            return X # Return original features if predictions are not valid arrays\n",
        "\n",
        "        try:\n",
        "            swap_pred = np.swapaxes(prf_crf_pred, 0, 1)\n",
        "            add_feat = swap_pred.reshape([X.shape[0], -1])\n",
        "            return np.concatenate([add_feat, X], axis=1)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating feature array: {e}. Returning original features.\")\n",
        "            return X\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # Ensure all attributes are included in get_params\n",
        "        return {'shape_1X': self.shape_1X,\n",
        "                'n_mgsRFtree': self.n_mgsRFtree,\n",
        "                'window': self.window,\n",
        "                'stride': self.stride,\n",
        "                'cascade_test_size': self.cascade_test_size,\n",
        "                'n_cascadeRF': self.n_cascadeRF,\n",
        "                'n_cascadeRFtree': self.n_cascadeRFtree,\n",
        "                'cascade_layer': self.cascade_layer,\n",
        "                'min_samples_mgs': self.min_samples_mgs,\n",
        "                'min_samples_cascade': self.min_samples_cascade,\n",
        "                'tolerance': self.tolerance,\n",
        "                'n_jobs': self.n_jobs,\n",
        "                'use_mg_scanning': self.use_mg_scanning}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "\n",
        "AAMI_CLASSES = {\n",
        "    'N': 0, 'L': 0, 'R': 0, 'e': 0, 'j': 0,\n",
        "    'A': 1, 'a': 1, 'J': 1, 'S': 1,\n",
        "    'V': 2, 'E': 2,\n",
        "    'F': 3,\n",
        "    '/': 4, 'f': 4, 'Q': 4,\n",
        "}\n",
        "\n",
        "def get_aami_class(symbol):\n",
        "    return AAMI_CLASSES.get(symbol)\n",
        "\n",
        "def apply_bandpass_filter(signal, fs=360):\n",
        "    lowcut = 0.5\n",
        "    highcut = 45.0\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(2, [low, high], btype='band')\n",
        "    return filtfilt(b, a, signal)\n",
        "\n",
        "def apply_denoising(signal, strategy='None', window_size=5):\n",
        "    if strategy is None or strategy.lower() == 'none':\n",
        "        return signal\n",
        "    elif strategy.lower() == 'moving_average':\n",
        "        # Ensure window_size is odd for centered filter\n",
        "        if window_size % 2 == 0:\n",
        "            window_size += 1\n",
        "        return uniform_filter1d(signal, size=window_size)\n",
        "    elif strategy.lower() == 'median':\n",
        "         # Ensure window_size is odd for median filter\n",
        "        if window_size % 2 == 0:\n",
        "            window_size += 1\n",
        "        return medfilt(signal, kernel_size=window_size)\n",
        "    # Add other denoising strategies here (e.g., wavelet denoising)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown denoising strategy: {strategy}. Choose from 'moving_average', 'median', or 'None'.\")\n",
        "\n",
        "\n",
        "def segment_heartbeats(signal, annotations, fs=360, window_size=360):\n",
        "    heartbeats, labels = [], []\n",
        "    window_before = window_size // 2\n",
        "    window_after = window_size - window_before\n",
        "    for i, symbol in enumerate(annotations.symbol):\n",
        "        aami_class = get_aami_class(symbol)\n",
        "        if aami_class is not None:\n",
        "            peak_sample = annotations.sample[i]\n",
        "            start, end = peak_sample - window_before, peak_sample + window_after\n",
        "            if start >= 0 and end < len(signal):\n",
        "                heartbeats.append(signal[start:end])\n",
        "                labels.append(aami_class)\n",
        "    return np.array(heartbeats), np.array(labels)\n",
        "\n",
        "def preprocess_data(data_path, window_size=360, max_records=None, balancing_strategy='SMOTE', denoising_strategy='None', denoising_window_size=5):\n",
        "    print(f\"Starting data preprocessing with balancing strategy: {balancing_strategy} and denoising strategy: {denoising_strategy}...\")\n",
        "\n",
        "    # Add error handling for data_path existence\n",
        "    if not os.path.exists(data_path):\n",
        "        raise FileNotFoundError(f\"Data path not found: {data_path}\")\n",
        "    if not os.path.isdir(data_path):\n",
        "         raise NotADirectoryError(f\"Data path is not a directory: {data_path}\")\n",
        "\n",
        "    record_names = sorted([f.split('.')[0] for f in os.listdir(data_path) if f.endswith('.hea')])\n",
        "    all_heartbeats, all_labels = [], []\n",
        "    for i, record_name in enumerate(record_names):\n",
        "        if max_records and i >= max_records:\n",
        "            break\n",
        "        try:\n",
        "            record = wfdb.rdrecord(os.path.join(data_path, record_name))\n",
        "            annotations = wfdb.rdann(os.path.join(data_path, record_name), 'atr')\n",
        "            signal = record.p_signal[:, record.sig_name.index('MLII') if 'MLII' in record.sig_name else 0]\n",
        "\n",
        "            # Apply denoising before filtering if desired, or after. Let's apply after bandpass for now.\n",
        "            filtered_signal = apply_bandpass_filter(signal, fs=record.fs)\n",
        "            denoised_signal = apply_denoising(filtered_signal, strategy=denoising_strategy, window_size=denoising_window_size)\n",
        "\n",
        "\n",
        "            heartbeats, labels = segment_heartbeats(denoised_signal, annotations, fs=record.fs, window_size=window_size)\n",
        "            all_heartbeats.append(heartbeats)\n",
        "            all_labels.append(labels)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process record {record_name}: {e}\")\n",
        "    if not all_heartbeats:\n",
        "        raise ValueError(\"No heartbeats processed. Check data path and file integrity.\")\n",
        "    X, y = np.concatenate(all_heartbeats), np.concatenate(all_labels)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    if balancing_strategy == 'SMOTE':\n",
        "        print(\"Applying SMOTE to balance the training data...\")\n",
        "        # Reduced k_neighbors to handle small classes, as seen in previous attempts\n",
        "        smote = SMOTE(random_state=42, k_neighbors=1)\n",
        "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "        print(f\"Original training samples: {len(y_train)}, Resampled training samples: {len(y_train_resampled)}\")\n",
        "    elif balancing_strategy == 'ADASYN':\n",
        "        print(\"Applying ADASYN to balance the training data...\")\n",
        "        # Set k_neighbors to a small value to handle very small minority classes\n",
        "        # Reduced k_neighbors to 1 to handle very small sample sizes in minority classes\n",
        "        adasyn = ADASYN(random_state=42, n_neighbors=1)\n",
        "        X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
        "        print(f\"Original training samples: {len(y_train)}, Resampled training samples: {len(y_train_resampled)}\")\n",
        "    elif balancing_strategy is None or balancing_strategy == 'None' or balancing_strategy.lower() == 'none':\n",
        "        print(\"Skipping data balancing.\")\n",
        "        X_train_resampled, y_train_resampled = X_train, y_train\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown balancing strategy: {balancing_strategy}. Choose from 'SMOTE', 'ADASYN', or 'None'.\")\n",
        "\n",
        "    print(\"Data preprocessing complete.\")\n",
        "    return X_train_resampled, X_test, y_train_resampled, y_test\n",
        "\n",
        "\n",
        "def extract_features(train_data, test_data, method='MFCC', wavelet='db4', level=4, padding_strategy='pad', target_length=None, scaling_strategy='None'):\n",
        "    print(f\"Extracting features using {method} method...\")\n",
        "    if method == 'MFCC':\n",
        "        train_features = _extract_mfcc(train_data)\n",
        "        test_features = _extract_mfcc(test_data)\n",
        "    elif method == 'DWT':\n",
        "        train_features = _extract_dwt(train_data, wavelet=wavelet, level=level, padding_strategy=padding_strategy, target_length=target_length)\n",
        "        test_features = _extract_dwt(test_data, wavelet=wavelet, level=level, padding_strategy=padding_strategy, target_length=target_length)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown feature extraction method: {method}\")\n",
        "    print(\"Feature extraction complete.\")\n",
        "\n",
        "    if scaling_strategy is not None and scaling_strategy.lower() != 'none':\n",
        "        print(f\"Applying {scaling_strategy} scaling...\")\n",
        "        if scaling_strategy == 'standard':\n",
        "            scaler = StandardScaler()\n",
        "        elif scaling_strategy == 'minmax':\n",
        "            scaler = MinMaxScaler()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown scaling strategy: {scaling_strategy}. Choose from 'standard', 'minmax', or 'None'.\")\n",
        "\n",
        "        # Fit on training data and transform both training and test data\n",
        "        train_features = scaler.fit_transform(train_features)\n",
        "        test_features = scaler.transform(test_features)\n",
        "        print(\"Scaling complete.\")\n",
        "    else:\n",
        "        print(\"Skipping feature scaling.\")\n",
        "\n",
        "    return train_features, test_features\n",
        "\n",
        "def _extract_mfcc(data, sr=360, n_mfcc=13):\n",
        "    # Adjusted n_fft to be less than or equal to the signal length (360)\n",
        "    mfccs = [np.mean(librosa.feature.mfcc(y=heartbeat.astype(float), sr=sr, n_mfcc=n_mfcc, n_fft=256).T, axis=0) for heartbeat in data]\n",
        "    return np.array(mfccs)\n",
        "\n",
        "def _extract_dwt(data, wavelet='db4', level=4, padding_strategy='pad', target_length=None):\n",
        "    coeffs = [pywt.wavedec(heartbeat, wavelet, level=level) for heartbeat in data]\n",
        "    flat_features = [np.concatenate([c.flatten() for c in coef]) for coef in coeffs]\n",
        "\n",
        "    if padding_strategy == 'pad':\n",
        "        max_len = max(len(f) for f in flat_features)\n",
        "        processed_features = np.array([np.pad(f, (0, max_len - len(f))) for f in flat_features])\n",
        "    elif padding_strategy == 'truncate':\n",
        "        if target_length is None:\n",
        "            raise ValueError(\"target_length must be specified for 'truncate' strategy\")\n",
        "        processed_features = np.array([f[:target_length] for f in flat_features])\n",
        "    elif padding_strategy == 'resize':\n",
        "        if target_length is None:\n",
        "            raise ValueError(\"target_length must be specified for 'resize' strategy\")\n",
        "        processed_features = []\n",
        "        for f in flat_features:\n",
        "            x = np.linspace(0, 1, len(f))\n",
        "            f_interp = interp1d(x, f)\n",
        "            x_new = np.linspace(0, 1, target_length)\n",
        "            processed_features.append(f_interp(x_new))\n",
        "        processed_features = np.array(processed_features)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown padding strategy: {padding_strategy}\")\n",
        "\n",
        "    return processed_features\n",
        "\n",
        "def train_and_evaluate(train_features, train_labels, test_features, test_labels, model_type='gcForest'):\n",
        "    print(f\"--- Training and evaluating {model_type} model ---\")\n",
        "    if model_type == 'CascadeForest':\n",
        "        param_grid = {\n",
        "            'n_cascadeRFtree': [101, 151], 'n_cascadeRF': [2],\n",
        "            'min_samples_cascade': [0.05, 0.1], 'cascade_layer': [15, 25], 'tolerance': [0.005]\n",
        "        }\n",
        "        model_base = gcForest(use_mg_scanning=False, n_jobs=-1)\n",
        "    elif model_type == 'gcForest':\n",
        "        feature_dim = train_features.shape[1]\n",
        "        param_grid = {\n",
        "            'window': [[int(feature_dim * 0.2)], [int(feature_dim * 0.3)]], 'n_mgsRFtree': [30],\n",
        "            'n_cascadeRFtree': [101], 'n_cascadeRF': [2], 'cascade_layer': [15], 'tolerance': [0.005]\n",
        "        }\n",
        "        # Pass shape_1X and n_jobs separately, let GridSearchCV handle the rest of the params\n",
        "        model_base = gcForest(shape_1X=train_features.shape[1], n_jobs=-1)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid model type: {model_type}\")\n",
        "    cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
        "    grid_search = GridSearchCV(estimator=model_base, param_grid=param_grid, cv=cv, n_jobs=-1, verbose=1, scoring='precision')\n",
        "    grid_search.fit(train_features, train_labels)\n",
        "    print(f\"Best parameters for {model_type}: {grid_search.best_params_}\")\n",
        "    model = grid_search.best_estimator_\n",
        "    print(\"Evaluating the best model on the test set...\")\n",
        "    predictions = model.predict(test_features)\n",
        "    probas = model.predict_proba(test_features)\n",
        "    print(f\"Accuracy: {accuracy_score(test_labels, predictions):.4f}\")\n",
        "    print(f\"F1-score: {f1_score(test_labels, predictions, average='weighted'):.4f}\")\n",
        "    print(f\"Precision: {precision_score(test_labels, predictions, average='weighted'):.4f}\")\n",
        "    print(f\"Recall: {recall_score(test_labels, predictions, average='weighted'):.4f}\")\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(test_labels, probas, multi_class='ovr', average='weighted')\n",
        "        print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Could not compute ROC AUC Score: {e}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(test_labels, predictions))\n",
        "    return model\n",
        "\n",
        "def explain_model(model, test_features, feature_names, output_path):\n",
        "    print(\"Calculating SHAP values...\")\n",
        "    try:\n",
        "        background_data = shap.sample(test_features, 100)\n",
        "        explainer = shap.KernelExplainer(model.predict_proba, background_data)\n",
        "        shap_values = explainer.shap_values(test_features)\n",
        "        print(\"Generating SHAP summary plot...\")\n",
        "        plt.figure()\n",
        "        if isinstance(shap_values, list):\n",
        "            # For multiclass, shap_values is a list of arrays, one for each class.\n",
        "            # A common way to visualize is to plot the mean absolute SHAP value across all classes\n",
        "            # or plot for a specific class (e.g., class 0).\n",
        "            # Here, let's plot for the first class as an example.\n",
        "            shap.summary_plot(shap_values[0], test_features, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
        "        else:\n",
        "            # For binary classification, shap_values is a single array\n",
        "            shap.summary_plot(shap_values, test_features, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
        "        plt.title(\"SHAP Feature Importance\")\n",
        "        plot_file = os.path.join(output_path, 'shap_summary_plot.png')\n",
        "        plt.savefig(plot_file)\n",
        "        plt.close()\n",
        "        print(f\"SHAP summary plot saved to {plot_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate SHAP plot: {e}\")\n",
        "\n",
        "\n",
        "def run_experiment(args):\n",
        "    print(f\"====================--- Starting Experiment: Model={args.model}, Features={args.feature_extractor}, Balancing={args.balancing_strategy}, Scaling={args.scaling_strategy}, Denoising={args.denoising_strategy} ---\")\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = preprocess_data(args.data_path, max_records=args.max_records, balancing_strategy=args.balancing_strategy, denoising_strategy=args.denoising_strategy, denoising_window_size=args.denoising_window_size)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error during preprocessing: {e}\")\n",
        "        print(\"--- Experiment Failed ---\")\n",
        "        return\n",
        "    except NotADirectoryError as e:\n",
        "        print(f\"Error during preprocessing: {e}\")\n",
        "        print(\"--- Experiment Failed ---\")\n",
        "        return\n",
        "    except ValueError as e:\n",
        "        print(f\"Error during preprocessing: {e}\")\n",
        "        print(\"--- Experiment Failed ---\")\n",
        "        return\n",
        "\n",
        "\n",
        "    train_features, test_features = extract_features(X_train, X_test, method=args.feature_extractor, wavelet=args.wavelet, level=args.level, padding_strategy=args.padding_strategy, target_length=args.target_length, scaling_strategy=args.scaling_strategy)\n",
        "    model = train_and_evaluate(train_features, y_train, test_features, y_test, model_type=args.model)\n",
        "    if args.explain:\n",
        "        feature_names = [f'{args.feature_extractor}_{i}' for i in range(train_features.shape[1])]\n",
        "        explain_model(model, test_features, feature_names, args.output_path)\n",
        "    print(f\"--- Experiment Finished: Model={args.model} ---\")\n",
        "\n",
        "# Define DATA_PATH and OUTPUT_PATH before calling run_experiment\n",
        "PROJECT_PATH = '/content/drive/MyDrive/MScUEL'\n",
        "DATA_PATH = os.path.join(PROJECT_PATH, 'mit-bih-arrhythmia-database-1.0.0')\n",
        "OUTPUT_PATH = os.path.join(PROJECT_PATH, 'colab_outputs')\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "args_cascade = argparse.Namespace(\n",
        "    data_path=DATA_PATH,\n",
        "    output_path=OUTPUT_PATH,\n",
        "    feature_extractor='MFCC',\n",
        "    model='CascadeForest',\n",
        "    explain=False,\n",
        "    max_records=4,\n",
        "    balancing_strategy='SMOTE',\n",
        "    scaling_strategy='standard',\n",
        "    denoising_strategy='moving_average', # Added denoising strategy\n",
        "    denoising_window_size=5, # Added denoising window size\n",
        "    # Add DWT-related arguments with default values\n",
        "    wavelet='db4',\n",
        "    level=4,\n",
        "    padding_strategy='pad',\n",
        "    target_length=None\n",
        ")\n",
        "run_experiment(args_cascade)\n",
        "\n",
        "# Example usage with ADASYN and standard scaling and median denoising\n",
        "# args_gc = argparse.Namespace(\n",
        "#     data_path=DATA_PATH,\n",
        "#     output_path=OUTPUT_PATH,\n",
        "#     feature_extractor='DWT',\n",
        "#     wavelet='db4', # Add wavelet arg for DWT\n",
        "#     level=4, # Add level arg for DWT\n",
        "#     padding_strategy='pad', # Add padding strategy arg for DWT\n",
        "#     target_length=None, # Add target length arg for DWT\n",
        "#     model='gcForest',\n",
        "#     explain=False,\n",
        "#     max_records=4,\n",
        "#     balancing_strategy='ADASYN', # Using ADASYN balancing\n",
        "#     scaling_strategy='standard', # Added scaling strategy\n",
        "#     denoising_strategy='median', # Added denoising strategy\n",
        "#     denoising_window_size=5 # Added denoising window size\n",
        "# )\n",
        "# run_experiment(args_gc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fb666b5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `n_fft` parameter in the `_extract_mfcc` function was successfully adjusted to `256` from a larger value, resolving the associated warning as 256 is less than the signal length (360) and a power of 2.\n",
        "*   The `_extract_dwt` and `extract_features` functions were modified to accept and utilize `wavelet` and `level` parameters, enabling experimentation with different Discrete Wavelet Transform configurations.\n",
        "*   Functionality was added to the `_extract_dwt` function to implement padding, truncation, or resizing of the extracted DWT features based on the specified `padding_strategy` and `target_length`.\n",
        "*   A denoising step was successfully integrated into the `preprocess_data` function, allowing for the application of different denoising strategies ('moving_average', 'median', or 'None') using a specified `denoising_window_size`.\n",
        "*   The experiment pipeline execution failed in subsequent steps due to a `FileNotFoundError` related to the data path, which was not caused by the implemented preprocessing improvements.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Verify and correct the `DATA_PATH` to ensure the experiment pipeline can run successfully and evaluate the impact of the implemented preprocessing improvements (denoising, DWT padding/truncation/resizing, different wavelets/levels) on model performance.\n",
        "*   Conduct systematic experiments varying the implemented preprocessing parameters (e.g., denoising strategy and window size, DWT wavelet, level, and padding strategy/target length) to determine the optimal combination for the ECG classification task.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "deepforest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f65ed5f9ecf43d894908fbdd2ba8a0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a31245b0023e43abb3e0ca07aa8baf38",
            "max": 1681,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d41a931fc3044323b63ae459b037a06e",
            "value": 101
          }
        },
        "7196331b3b0e4d3db45847a6ced2e882": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73027e0125044690bbbd738adeedc95f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d93f99c08ebf4cceb2b45abd23ad1124",
            "placeholder": "​",
            "style": "IPY_MODEL_f523ce6b16d34ea09a4c8e20098df3ff",
            "value": " 101/1681 [10:12&lt;3:00:17,  6.85s/it]"
          }
        },
        "8ea5651b878744b48f30343ff00c51aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94e060259f094a0b8c4e6c2db2147f77": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a31245b0023e43abb3e0ca07aa8baf38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b56dd29c0a76461fb8331bb08d90dbde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7196331b3b0e4d3db45847a6ced2e882",
            "placeholder": "​",
            "style": "IPY_MODEL_8ea5651b878744b48f30343ff00c51aa",
            "value": "  6%"
          }
        },
        "d19fe35244d54ca4b1cc99470a6a19cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b56dd29c0a76461fb8331bb08d90dbde",
              "IPY_MODEL_4f65ed5f9ecf43d894908fbdd2ba8a0e",
              "IPY_MODEL_73027e0125044690bbbd738adeedc95f"
            ],
            "layout": "IPY_MODEL_94e060259f094a0b8c4e6c2db2147f77"
          }
        },
        "d41a931fc3044323b63ae459b037a06e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d93f99c08ebf4cceb2b45abd23ad1124": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f523ce6b16d34ea09a4c8e20098df3ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}