{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep FECG Research: All-in-One Setup and Test Notebook\n",
    "\n",
    "This notebook contains all the commands for setting up your environment and running a test of the `deep-fecg-research` project. Please read the instructions carefully, especially regarding environment activation.\n",
    "\n",
    "**IMPORTANT:** While all commands are listed here, the `pyenv activate` command *must* be run in your terminal *before* you launch Jupyter Notebook or JupyterLab. Running it within a notebook cell will not correctly activate the environment for the Jupyter kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Activate Python Environment (Run this in your Terminal FIRST!)\n",
    "\n",
    "**Do NOT run this cell in Jupyter.** Copy and paste this command into your terminal and execute it there. Then, from the *same terminal*, launch Jupyter Notebook or JupyterLab.\n",
    "\n",
    "```bash\n",
    "pyenv activate deepforest\n",
    "```\n",
    "\n",
    "Once you have activated the environment and launched Jupyter, you can proceed with the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for demonstration purposes only. \n",
    "# It will NOT activate the environment for subsequent cells in Jupyter.\n",
    "# You MUST run 'pyenv activate deepforest' in your terminal before starting Jupyter.\n",
    "# !pyenv activate deepforest # Uncomment and run in terminal, not here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install `uv` (if not already installed)\n",
    "\n",
    "`uv` is a fast Python package installer and resolver. We'll use it to manage project dependencies. Run this cell to install `uv` into your active environment.\n",
    "\n",
    "*(The `!` prefix runs the command in the shell from within Jupyter.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install/Reinstall Project Dependencies with `uv`\n",
    "\n",
    "The `numpy.dtype` size error often indicates a binary incompatibility. To resolve this, we'll force a reinstallation of all project dependencies using `uv`. The `--active` flag ensures `uv` targets your currently active `pyenv` environment.\n",
    "\n",
    "*(This step might take a few moments as it downloads and reinstalls packages.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --force-reinstall -r requirements.txt --active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Project Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "def load_config(config_path='config.yaml'):\n",
    "    \"\"\"\n",
    "    Loads a YAML configuration file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Configuration file not found at {config_path}\")\n",
    "    with open(config_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def save_results(results, output_path='results.txt'):\n",
    "    \"\"\"\n",
    "    Saves the experiment results to a text file.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w') as file:\n",
    "        for key, value in results.items():\n",
    "            file.write(f'{key}: {value}\n')\n",
    "\n",
    "def check_and_create_dir(directory):\n",
    "    \"\"\"\n",
    "    Checks if a directory exists, and if not, creates it.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Creating directory: {directory}\")\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wfdb\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# AAMI-compliant class mappings\n",
    "AAMI_CLASSES = {\n",
    "    'N': 0, 'L': 0, 'R': 0, 'e': 0, 'j': 0,  # Non-ectopic\n",
    "    'A': 1, 'a': 1, 'J': 1, 'S': 1,         # Supraventricular ectopic\n",
    "    'V': 2, 'E': 2,                         # Ventricular ectopic\n",
    "    'F': 3,                                 # Fusion\n",
    "    '/': 4, 'f': 4, 'Q': 4,                 # Paced/Unknown\n",
    "}\n",
    "\n",
    "def get_aami_class(symbol):\n",
    "    \"\"\"Maps an annotation symbol to its AAMI class.\"\"\"\n",
    "    return AAMI_CLASSES.get(symbol)\n",
    "\n",
    "def apply_bandpass_filter(signal, fs=360):\n",
    "    \"\"\"Applies a band-pass filter to the signal.\"\"\"\n",
    "    lowcut = 0.5\n",
    "    highcut = 45.0\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(2, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def segment_heartbeats(signal, annotations, fs=360, window_size=360):\n",
    "    \"\"\"\n",
    "    Segments the signal into individual heartbeats.\n",
    "    \"\"\"\n",
    "    heartbeats = []\n",
    "    labels = []\n",
    "    window_before = window_size // 2\n",
    "    window_after = window_size - window_before\n",
    "\n",
    "    for i, symbol in enumerate(annotations.symbol):\n",
    "        aami_class = get_aami_class(symbol)\n",
    "        if aami_class is not None:\n",
    "            peak_sample = annotations.sample[i]\n",
    "            start = peak_sample - window_before\n",
    "            end = peak_sample + window_after\n",
    "            if start >= 0 and end < len(signal):\n",
    "                heartbeats.append(signal[start:end])\n",
    "                labels.append(aami_class)\n",
    "\n",
    "    return np.array(heartbeats), np.array(labels)\n",
    "\n",
    "def preprocess_data(data_path, window_size=360, max_records=None):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the ECG data from the MIT-BIH Arrhythmia Database.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {data_path}...\")\n",
    "    \n",
    "    # Get a list of all record names by listing .hea files\n",
    "    record_names = [f.split('.')[0] for f in os.listdir(data_path) if f.endswith('.hea')]\n",
    "    record_names.sort() # Ensure consistent order\n",
    "\n",
    "    all_heartbeats = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, record_name in enumerate(record_names):\n",
    "        if max_records and i >= max_records:\n",
    "            print(f\"Reached max_records limit of {max_records}. Stopping data loading.\")\n",
    "            break\n",
    "        print(f\"Processing record: {record_name}\")\n",
    "        record_full_path = os.path.join(data_path, record_name)\n",
    "        try:\n",
    "            record = wfdb.rdrecord(record_full_path)\n",
    "            annotations = wfdb.rdann(record_full_path, 'atr')\n",
    "\n",
    "            # Use the first channel (MLII) if available, otherwise the first channel\n",
    "            if 'MLII' in record.sig_name:\n",
    "                signal_index = record.sig_name.index('MLII')\n",
    "            else:\n",
    "                signal_index = 0 # Default to first channel\n",
    "            signal = record.p_signal[:, signal_index]\n",
    "\n",
    "            # Apply band-pass filter\n",
    "            filtered_signal = apply_bandpass_filter(signal, fs=record.fs)\n",
    "\n",
    "            # Segment heartbeats\n",
    "            heartbeats, labels = segment_heartbeats(\n",
    "                filtered_signal, annotations, fs=record.fs, window_size=window_size\n",
    "            )\n",
    "\n",
    "            all_heartbeats.append(heartbeats)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing record {record_name}: {e}\")\n",
    "\n",
    "    if not all_heartbeats:\n",
    "        raise ValueError(\"No heartbeats processed. Check data_path and file integrity.\")\n",
    "\n",
    "    X = np.concatenate(all_heartbeats)\n",
    "    y = np.concatenate(all_labels)\n",
    "\n",
    "    print(\"Splitting data into training and testing sets...\")\n",
    "    # Stratified 80/20 split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import pywt\n",
    "from ssqueezepy import ssq_cwt\n",
    "\n",
    "def extract_features(train_data, test_data, method='MFCC'):\n",
    "    \"\"\"\n",
    "    Extracts features from the preprocessed ECG data.\n",
    "\n",
    "    Args:\n",
    "        train_data (np.ndarray): Training data (heartbeats).\n",
    "        test_data (np.ndarray): Testing data (heartbeats).\n",
    "        method (str): Feature extraction method (MFCC, DWT, HHT, SSCWT).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing train_features, test_features.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting features using {method} method...\")\n",
    "\n",
    "    if method == 'MFCC':\n",
    "        train_features = _extract_mfcc(train_data)\n",
    "        test_features = _extract_mfcc(test_data)\n",
    "    elif method == 'DWT':\n",
    "        train_features = _extract_dwt(train_data)\n",
    "        test_features = _extract_dwt(test_data)\n",
    "    elif method == 'HHT':\n",
    "        train_features = _extract_hht(train_data)\n",
    "        test_features = _extract_hht(test_data)\n",
    "    elif method == 'SSCWT':\n",
    "        train_features = _extract_sscwt(train_data)\n",
    "        test_features = _extract_sscwt(test_data)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown feature extraction method: {method}\")\n",
    "\n",
    "    return train_features, test_features\n",
    "\n",
    "def _extract_mfcc(data, sr=360, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Extracts Mel-frequency cepstral coefficients (MFCCs).\n",
    "    \"\"\"\n",
    "    mfccs = []\n",
    "    for heartbeat in data:\n",
    "        # Ensure heartbeat is float type for librosa\n",
    "        heartbeat = heartbeat.astype(float)\n",
    "        mfcc = librosa.feature.mfcc(y=heartbeat, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfccs.append(np.mean(mfcc.T, axis=0)) # Take mean across time frames\n",
    "    return np.array(mfccs)\n",
    "\n",
    "def _extract_dwt(data, wavelet='db4', level=4):\n",
    "    \"\"\"\n",
    "    Extracts Discrete Wavelet Transform (DWT) features.\n",
    "    \"\"\"\n",
    "    dwt_features = []\n",
    "    for heartbeat in data:\n",
    "        coeffs = pywt.wavedec(heartbeat, wavelet, level=level)\n",
    "        # Flatten coefficients and concatenate them\n",
    "        features = np.concatenate([np.array(c).flatten() for c in coeffs])\n",
    "        dwt_features.append(features)\n",
    "    # Pad features to the maximum length if they are not uniform\n",
    "    max_len = max(len(f) for f in dwt_features)\n",
    "    padded_features = np.array([np.pad(f, (0, max_len - len(f)), 'constant') for f in dwt_features])\n",
    "    return padded_features\n",
    "\n",
    "def _extract_hht(data):\n",
    "    \"\"\"\n",
    "    Extracts Hilbert-Huang Transform (HHT) features.\n",
    "    Note: HHT implementation is complex and often requires external libraries\n",
    "    or a custom implementation of EMD. This is a placeholder.\n",
    "    For a full implementation, consider libraries like `emd`.\n",
    "    \"\"\"\n",
    "    print(\"Warning: HHT feature extraction is a placeholder and returns dummy data.\")\n",
    "    # Dummy implementation: return mean and std of the signal as basic features\n",
    "    hht_features = []\n",
    "    for heartbeat in data:\n",
    "        hht_features.append([np.mean(heartbeat), np.std(heartbeat)])\n",
    "    return np.array(hht_features)\n",
    "\n",
    "def _extract_sscwt(data, fs=360):\n",
    "    \"\"\"\n",
    "    Extracts Synchrosqueezed Continuous Wavelet Transform (SSCWT) features.\n",
    "    Note: SSCWT can produce high-dimensional output. This is a placeholder\n",
    "    and returns a simplified representation.\n",
    "    \"\"\"\n",
    "    print(\"Warning: SSCWT feature extraction is a placeholder and returns dummy data.\")\n",
    "    sscwt_features = []\n",
    "    for heartbeat in data:\n",
    "        # ssq_cwt returns (Tx, Wx, ssq_freqs, scales, wavel_scales)\n",
    "        # We'll take the mean of the absolute value of the transform as a simple feature\n",
    "        Tx, Wx, ssq_freqs, scales, wavel_scales = ssq_cwt(heartbeat, 'morlet', fs=fs)\n",
    "        sscwt_features.append(np.mean(np.abs(Tx)))\n",
    "    return np.array(sscwt_features).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from deepforest import CascadeForestClassifier\n",
    "from gcforest.gcforest import GCForest\n",
    "\n",
    "def train_and_evaluate(train_features, train_labels, test_features, test_labels, model_type='gcForest'):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the specified Deep Forest model.\n",
    "\n",
    "    Args:\n",
    "        train_features (np.ndarray): The training features.\n",
    "        train_labels (np.ndarray): The training labels.\n",
    "        test_features (np.ndarray): The testing features.\n",
    "        test_labels (np.ndarray): The testing labels.\n",
    "        model_type (str): The type of model to train ('gcForest' or 'CascadeForest').\n",
    "\n",
    "    Returns:\n",
    "        object: The trained model.\n",
    "    \"\"\"\n",
    "    print(f\"Training {model_type} model...\")\n",
    "\n",
    "    if model_type == 'gcForest':\n",
    "        model = _train_gcforest(train_features, train_labels)\n",
    "    elif model_type == 'CascadeForest':\n",
    "        model = _train_cascade_forest(train_features, train_labels)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model type: {model_type}\")\n",
    "\n",
    "    print(\"Evaluating the model...\")\n",
    "    predictions = model.predict(test_features)\n",
    "\n",
    "    # Calculate and print performance metrics\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "    precision = precision_score(test_labels, predictions, average='weighted')\n",
    "    recall = recall_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def _train_gcforest(train_features, train_labels):\n",
    "    print(\"Training gcForest...\")\n",
    "    # Initialize GCForest with some default parameters\n",
    "    # These parameters can be tuned further for optimal performance\n",
    "    gc = GCForest(shape_1X=train_features.shape[1],\n",
    "                  n_mgs=1,\n",
    "                  n_estimators_as_forest=[100],\n",
    "                  min_samples_leaf=1,\n",
    "                  max_depth=None,\n",
    "                  n_tolerant_retry=10,\n",
    "                  n_jobs=-1) # Use all available cores\n",
    "    gc.fit(train_features, train_labels)\n",
    "    return gc\n",
    "\n",
    "def _train_cascade_forest(train_features, train_labels):\n",
    "    print(\"Training CascadeForestClassifier...\")\n",
    "    # Initialize CascadeForestClassifier with some default parameters\n",
    "    # These parameters can be tuned further for optimal performance\n",
    "    cf = CascadeForestClassifier(n_estimators=100,\n",
    "                                 n_trees=500,\n",
    "                                 use_predictor=True,\n",
    "                                 min_samples_leaf=1,\n",
    "                                 max_depth=None,\n",
    "                                 n_jobs=-1) # Use all available cores\n",
    "    cf.fit(train_features, train_labels)\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def explain_model(model, test_features, test_labels):\n",
    "    \"\"\"\n",
    "    Explains the model's predictions using SHAP.\n",
    "\n",
    "    Args:\n",
    "        model (object): The trained model (gcForest or CascadeForestClassifier).\n",
    "        test_features (np.ndarray): The testing features.\n",
    "        test_labels (np.ndarray): The testing labels.\n",
    "    \"\"\"\n",
    "    print(\"Calculating SHAP values...\")\n",
    "\n",
    "    # For tree-based models like gcForest and CascadeForestClassifier, TreeExplainer is efficient.\n",
    "    # If the model is a scikit-learn compatible tree ensemble, TreeExplainer should work.\n",
    "    # If not, KernelExplainer is a more general but slower alternative.\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(test_features)\n",
    "    except Exception as e:\n",
    "        print(f\"TreeExplainer failed: {e}. Falling back to KernelExplainer (may be slow)...\")\n",
    "        # KernelExplainer requires a background dataset for estimation\n",
    "        # Using a subset of test_features as background for performance\n",
    "        background_data = shap.sample(test_features, 100) # Sample 100 instances\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, background_data)\n",
    "        shap_values = explainer.shap_values(test_features)\n",
    "\n",
    "    print(\"Generating SHAP summary plot...\")\n",
    "    # If the model is multi-output (multi-class classification), shap_values will be a list of arrays.\n",
    "    # For summary_plot, we often plot for one class or the absolute mean of all classes.\n",
    "    if isinstance(shap_values, list):\n",
    "        # For multi-class, plot the SHAP values for the first class (or average/sum them)\n",
    "        shap.summary_plot(shap_values[0], test_features, plot_type=\"bar\", show=False)\n",
    "    else:\n",
    "        shap.summary_plot(shap_values, test_features, plot_type=\"bar\", show=False)\n",
    "\n",
    "    plt.title(\"SHAP Feature Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"shap_summary_plot.png\")\n",
    "    plt.close()\n",
    "    print(\"SHAP summary plot saved as shap_summary_plot.png\")\n",
    "\n",
    "    # You can also implement logic to analyze misclassified instances\n",
    "    # and generate local SHAP plots (e.g., shap.force_plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"\n",
    "    Main function to run the experiment.\n",
    "    \"\"\"\n",
    "    # 1. Preprocess the data\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    train_data, test_data, train_labels, test_labels = preprocess_data(args.data_path, max_records=args.max_records)\n",
    "    print(\"Data preprocessing complete.\")\n",
    "\n",
    "    # 2. Extract features\n",
    "    print(\"Extracting features...\")\n",
    "    train_features, test_features = extract_features(train_data, test_data, method=args.feature_extractor)\n",
    "    print(\"Feature extraction complete.\")\n",
    "\n",
    "    # 3. Train and evaluate the model\n",
    "    print(\"Training and evaluating the model...\")\n",
    "    model = train_and_evaluate(train_features, train_labels, test_features, test_labels, model_type=args.model)\n",
    "    print(\"Model training and evaluation complete.\")\n",
    "\n",
    "    # 4. Explain the model\n",
    "    if args.explain:\n",
    "        print(\"Explaining the model...\")\n",
    "        explain_model(model, test_features, test_labels)\n",
    "        print(\"Model explanation complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Run the Deep Forest ECG experiment.')\n",
    "    parser.add_argument('--data_path', type=str, default='./data', help='Path to the dataset.')\n",
    "    parser.add_argument('--feature_extractor', type=str, default='MFCC', choices=['MFCC', 'DWT', 'HHT', 'SSCWT'], help='Feature extraction method.')\n",
    "    parser.add_argument('--model', type=str, default='gcForest', choices=['gcForest', 'CascadeForest'], help='Model to train.')\n",
    "    parser.add_argument('--explain', action='store_true', help='Whether to run SHAP explainability.')\n",
    "    parser.add_argument('--max_records', type=int, default=None, help='Maximum number of records to process for testing purposes.')\n",
    "    args = parser.parse_args(args=[])\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run a Small Test\n",
    "\n",
    "Now that the dependencies should be correctly installed, we can run `main.py` with a small dataset to verify the setup. We'll use the `--max_records` argument to limit the data processed and `--feature_extractor MFCC`.\n",
    "\n",
    "*(This will execute your `main.py` script and print its output below the cell.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(data_path='./data/mit-bih-arrhythmia-database-1.0.0', feature_extractor='MFCC', model='gcForest', explain=False, max_records=10)\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If the last cell executed without the `numpy.dtype` error and showed output from `main.py` (e.g., \"Starting data preprocessing...\"), your environment is correctly set up for the `deep-fecg-research` project. You can now proceed with your research!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}