{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep FECG Research: All-in-One Experiment Notebook for Google Colab\n",
    "\n",
    "This notebook is optimized for Python 3.12+ and modern libraries in a Google Colab environment. It contains all the code for data preprocessing, feature extraction, and model training using a self-contained `gcForest` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "This cell installs all necessary libraries. Run it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip install -q wfdb librosa pywavelets ssqueezepy imbalanced-learn shap matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive & Define Paths\n",
    "\n",
    "This section mounts your Google Drive to make your dataset accessible. You will need to authorize Colab to access your Drive.\n",
    "\n",
    "**IMPORTANT:** After running the second cell, you **must** update the `PROJECT_PATH` variable to point to the correct location of your project folder on Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: Update this path to your project directory on Google Drive\n",
    "PROJECT_PATH = '/content/drive/MyDrive/deep_fecg_research'\n",
    "\n",
    "# --- You should not need to edit below this line ---\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data/mit-bih-arrhythmia-database-1.0.0')\n",
    "OUTPUT_PATH = os.path.join(PROJECT_PATH, 'colab_outputs')\n",
    "\n",
    "# Create an output directory for plots if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Project path set to: {PROJECT_PATH}\")\n",
    "print(f\"Data path set to: {DATA_PATH}\")\n",
    "print(f\"Output path set to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. All-in-One Experiment Code\n",
    "\n",
    "The following cells contain all the necessary code for the experiment pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class gcForest(object):\n",
    "    def __init__(self, shape_1X=None, n_mgsRFtree=30, window=None, stride=1,\n",
    "                 cascade_test_size=0.2, n_cascadeRF=2, n_cascadeRFtree=101, cascade_layer=np.inf,\n",
    "                 min_samples_mgs=0.1, min_samples_cascade=0.05, tolerance=0.0, n_jobs=1, use_mg_scanning=True):\n",
    "        self.shape_1X = shape_1X\n",
    "        self.n_layer = 0\n",
    "        self._n_samples = 0\n",
    "        self.n_cascadeRF = int(n_cascadeRF)\n",
    "        self.window = [window] if isinstance(window, int) else window\n",
    "        self.stride = stride\n",
    "        self.cascade_test_size = cascade_test_size\n",
    "        self.n_mgsRFtree = int(n_mgsRFtree)\n",
    "        self.n_cascadeRFtree = int(n_cascadeRFtree)\n",
    "        self.cascade_layer = cascade_layer\n",
    "        self.min_samples_mgs = min_samples_mgs\n",
    "        self.min_samples_cascade = min_samples_cascade\n",
    "        self.tolerance = tolerance\n",
    "        self.n_jobs = n_jobs\n",
    "        self.use_mg_scanning = use_mg_scanning\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if X.shape[0] != len(y):\n",
    "            raise ValueError('Sizes of y and X do not match.')\n",
    "        if self.use_mg_scanning:\n",
    "            X = self.mg_scanning(X, y)\n",
    "        self.cascade_forest(X, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.use_mg_scanning:\n",
    "            X = self.mg_scanning(X)\n",
    "        cascade_all_pred_prob = self.cascade_forest(X)\n",
    "        return np.mean(cascade_all_pred_prob, axis=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_proba = self.predict_proba(X=X)\n",
    "        return np.argmax(pred_proba, axis=1)\n",
    "\n",
    "    def mg_scanning(self, X, y=None):\n",
    "        self._n_samples = X.shape[0]\n",
    "        shape_1X = self.shape_1X\n",
    "        if isinstance(shape_1X, int):\n",
    "            shape_1X = [1, shape_1X]\n",
    "        if not self.window:\n",
    "            self.window = [shape_1X[1]]\n",
    "        mgs_pred_prob = []\n",
    "        for wdw_size in self.window:\n",
    "            wdw_pred_prob = self._window_slicing_pred_prob(X, wdw_size, shape_1X, y=y)\n",
    "            mgs_pred_prob.append(wdw_pred_prob)\n",
    "        return np.concatenate(mgs_pred_prob, axis=1)\n",
    "\n",
    "    def _window_slicing_pred_prob(self, X, window, shape_1X, y=None):\n",
    "        if shape_1X[0] > 1:\n",
    "            sliced_X, sliced_y = self._window_slicing_img(X, window, shape_1X, y=y, stride=self.stride)\n",
    "        else:\n",
    "            sliced_X, sliced_y = self._window_slicing_sequence(X, window, shape_1X, y=y, stride=self.stride)\n",
    "        if y is not None:\n",
    "            prf = RandomForestClassifier(n_estimators=self.n_mgsRFtree, max_features='sqrt', min_samples_split=self.min_samples_mgs, oob_score=True, n_jobs=self.n_jobs)\n",
    "            crf = RandomForestClassifier(n_estimators=self.n_mgsRFtree, max_features=1, min_samples_split=self.min_samples_mgs, oob_score=True, n_jobs=self.n_jobs)\n",
    "            prf.fit(sliced_X, sliced_y)\n",
    "            crf.fit(sliced_X, sliced_y)\n",
    "            setattr(self, f'_mgsprf_{window}', prf)\n",
    "            setattr(self, f'_mgscrf_{window}', crf)\n",
    "            pred_prob_prf = prf.oob_decision_function_\n",
    "            pred_prob_crf = crf.oob_decision_function_\n",
    "        else:\n",
    "            prf = getattr(self, f'_mgsprf_{window}')\n",
    "            crf = getattr(self, f'_mgscrf_{window}')\n",
    "            pred_prob_prf = prf.predict_proba(sliced_X)\n",
    "            pred_prob_crf = crf.predict_proba(sliced_X)\n",
    "        pred_prob = np.c_[pred_prob_prf, pred_prob_crf]\n",
    "        return pred_prob.reshape([self._n_samples, -1])\n",
    "\n",
    "    def _window_slicing_sequence(self, X, window, shape_1X, y=None, stride=1):\n",
    "        if shape_1X[1] < window:\n",
    "            raise ValueError('window must be smaller than the sequence dimension')\n",
    "        len_iter = (shape_1X[1] - window) // stride + 1\n",
    "        iter_array = np.arange(0, stride * len_iter, stride)\n",
    "        inds_to_take = [np.arange(i, i + window) for i in iter_array]\n",
    "        sliced_X = np.take(X, inds_to_take, axis=1).reshape(-1, window)\n",
    "        if y is not None:\n",
    "            sliced_y = np.repeat(y, len_iter)\n",
    "        else:\n",
    "            sliced_y = None\n",
    "        return sliced_X, sliced_y\n",
    "\n",
    "    def cascade_forest(self, X, y=None):\n",
    "        if y is not None:\n",
    "            self.n_layer = 0\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.cascade_test_size)\n",
    "            self.n_layer += 1\n",
    "            prf_crf_pred_ref = self._cascade_layer(X_train, y_train)\n",
    "            accuracy_ref = self._cascade_evaluation(X_test, y_test)\n",
    "            feat_arr = self._create_feat_arr(X_train, prf_crf_pred_ref)\n",
    "            self.n_layer += 1\n",
    "            prf_crf_pred_layer = self._cascade_layer(feat_arr, y_train)\n",
    "            accuracy_layer = self._cascade_evaluation(X_test, y_test)\n",
    "            while accuracy_layer > (accuracy_ref + self.tolerance) and self.n_layer <= self.cascade_layer:\n",
    "                accuracy_ref = accuracy_layer\n",
    "                prf_crf_pred_ref = prf_crf_pred_layer\n",
    "                feat_arr = self._create_feat_arr(X_train, prf_crf_pred_ref)\n",
    "                self.n_layer += 1\n",
    "                prf_crf_pred_layer = self._cascade_layer(feat_arr, y_train)\n",
    "                accuracy_layer = self._cascade_evaluation(X_test, y_test)\n",
    "            if accuracy_layer < accuracy_ref:\n",
    "                for irf in range(self.n_cascadeRF):\n",
    "                    delattr(self, f'_casprf{self.n_layer}_{irf}')\n",
    "                    delattr(self, f'_cascrf{self.n_layer}_{irf}')\n",
    "                self.n_layer -= 1\n",
    "        else:\n",
    "            at_layer = 1\n",
    "            prf_crf_pred_ref = self._cascade_layer(X, layer=at_layer)\n",
    "            while at_layer < self.n_layer:\n",
    "                at_layer += 1\n",
    "                feat_arr = self._create_feat_arr(X, prf_crf_pred_ref)\n",
    "                prf_crf_pred_ref = self._cascade_layer(feat_arr, layer=at_layer)\n",
    "        return prf_crf_pred_ref\n",
    "\n",
    "    def _cascade_layer(self, X, y=None, layer=0):\n",
    "        prf = RandomForestClassifier(n_estimators=self.n_cascadeRFtree, max_features='sqrt', min_samples_split=self.min_samples_cascade, oob_score=True, n_jobs=self.n_jobs)\n",
    "        crf = RandomForestClassifier(n_estimators=self.n_cascadeRFtree, max_features=1, min_samples_split=self.min_samples_cascade, oob_score=True, n_jobs=self.n_jobs)\n",
    "        prf_crf_pred = []\n",
    "        if y is not None:\n",
    "            for irf in range(self.n_cascadeRF):\n",
    "                prf.fit(X, y)\n",
    "                crf.fit(X, y)\n",
    "                setattr(self, f'_casprf{self.n_layer}_{irf}', prf)\n",
    "                setattr(self, f'_cascrf{self.n_layer}_{irf}', crf)\n",
    "                prf_crf_pred.append(prf.oob_decision_function_)\n",
    "                prf_crf_pred.append(crf.oob_decision_function_)\n",
    "        else:\n",
    "            for irf in range(self.n_cascadeRF):\n",
    "                prf = getattr(self, f'_casprf{layer}_{irf}')\n",
    "                crf = getattr(self, f'_cascrf{layer}_{irf}')\n",
    "                prf_crf_pred.append(prf.predict_proba(X))\n",
    "                prf_crf_pred.append(crf.predict_proba(X))\n",
    "        return prf_crf_pred\n",
    "\n",
    "    def _cascade_evaluation(self, X_test, y_test):\n",
    "        casc_pred_prob = np.mean(self.cascade_forest(X_test), axis=0)\n",
    "        casc_pred = np.argmax(casc_pred_prob, axis=1)\n",
    "        return accuracy_score(y_true=y_test, y_pred=casc_pred)\n",
    "\n",
    "    def _create_feat_arr(self, X, prf_crf_pred):\n",
    "        swap_pred = np.swapaxes(prf_crf_pred, 0, 1)\n",
    "        add_feat = swap_pred.reshape([X.shape[0], -1])\n",
    "        return np.concatenate([add_feat, X], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "AAMI_CLASSES = {\n",
    "    'N': 0, 'L': 0, 'R': 0, 'e': 0, 'j': 0, \n",
    "    'A': 1, 'a': 1, 'J': 1, 'S': 1, \n",
    "    'V': 2, 'E': 2, \n",
    "    'F': 3, \n",
    "    '/': 4, 'f': 4, 'Q': 4, \n",
    "}\n",
    "\n",
    "def get_aami_class(symbol):\n",
    "    return AAMI_CLASSES.get(symbol)\n",
    "\n",
    "def apply_bandpass_filter(signal, fs=360):\n",
    "    lowcut = 0.5\n",
    "    highcut = 45.0\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(2, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def segment_heartbeats(signal, annotations, fs=360, window_size=360):\n",
    "    heartbeats, labels = [], []\n",
    "    window_before = window_size // 2\n",
    "    window_after = window_size - window_before\n",
    "    for i, symbol in enumerate(annotations.symbol):\n",
    "        aami_class = get_aami_class(symbol)\n",
    "        if aami_class is not None:\n",
    "            peak_sample = annotations.sample[i]\n",
    "            start, end = peak_sample - window_before, peak_sample + window_after\n",
    "            if start >= 0 and end < len(signal):\n",
    "                heartbeats.append(signal[start:end])\n",
    "                labels.append(aami_class)\n",
    "    return np.array(heartbeats), np.array(labels)\n",
    "\n",
    "def preprocess_data(data_path, window_size=360, max_records=None):\n",
    "    print(f\"Starting data preprocessing...\")\n",
    "    record_names = sorted([f.split('.')[0] for f in os.listdir(data_path) if f.endswith('.hea')])\n",
    "    all_heartbeats, all_labels = [], []\n",
    "    for i, record_name in enumerate(record_names):\n",
    "        if max_records and i >= max_records:\n",
    "            break\n",
    "        try:\n",
    "            record = wfdb.rdrecord(os.path.join(data_path, record_name))\n",
    "            annotations = wfdb.rdann(os.path.join(data_path, record_name), 'atr')\n",
    "            signal = record.p_signal[:, record.sig_name.index('MLII') if 'MLII' in record.sig_name else 0]\n",
    "            filtered_signal = apply_bandpass_filter(signal, fs=record.fs)\n",
    "            heartbeats, labels = segment_heartbeats(filtered_signal, annotations, fs=record.fs, window_size=window_size)\n",
    "            all_heartbeats.append(heartbeats)\n",
    "            all_labels.append(labels)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process record {record_name}: {e}\")\n",
    "    if not all_heartbeats:\n",
    "        raise ValueError(\"No heartbeats processed. Check data path and file integrity.\")\n",
    "    X, y = np.concatenate(all_heartbeats), np.concatenate(all_labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(\"Applying SMOTE to balance the training data...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    print(f\"Original training samples: {len(y_train)}, Resampled training samples: {len(y_train_resampled)}\")\n",
    "    print(\"Data preprocessing complete.\")\n",
    "    return X_train_resampled, X_test, y_train_resampled, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pywt\n",
    "\n",
    "def extract_features(train_data, test_data, method='MFCC'):\n",
    "    print(f\"Extracting features using {method} method...\")\n",
    "    if method == 'MFCC':\n",
    "        train_features = _extract_mfcc(train_data)\n",
    "        test_features = _extract_mfcc(test_data)\n",
    "    elif method == 'DWT':\n",
    "        train_features = _extract_dwt(train_data)\n",
    "        test_features = _extract_dwt(test_data)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown feature extraction method: {method}\")\n",
    "    print(\"Feature extraction complete.\")\n",
    "    return train_features, test_features\n",
    "\n",
    "def _extract_mfcc(data, sr=360, n_mfcc=13):\n",
    "    mfccs = [np.mean(librosa.feature.mfcc(y=heartbeat.astype(float), sr=sr, n_mfcc=n_mfcc, n_fft=2048).T, axis=0) for heartbeat in data]\n",
    "    return np.array(mfccs)\n",
    "\n",
    "def _extract_dwt(data, wavelet='db4', level=4):\n",
    "    coeffs = [pywt.wavedec(heartbeat, wavelet, level=level) for heartbeat in data]\n",
    "    flat_features = [np.concatenate([c.flatten() for c in coef]) for coef in coeffs]\n",
    "    max_len = max(len(f) for f in flat_features)\n",
    "    padded_features = np.array([np.pad(f, (0, max_len - len(f))) for f in flat_features])\n",
    "    return padded_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "def train_and_evaluate(train_features, train_labels, test_features, test_labels, model_type='gcForest'):\n",
    "    print(f\"--- Training and evaluating {model_type} model ---\")\n",
    "    if model_type == 'CascadeForest':\n",
    "        param_grid = {\n",
    "            'n_cascadeRFtree': [101, 151], 'n_cascadeRF': [2],\n",
    "            'min_samples_cascade': [0.05, 0.1], 'cascade_layer': [15, 25], 'tolerance': [0.005]\n",
    "        }\n",
    "        model_base = gcForest(use_mg_scanning=False, n_jobs=-1)\n",
    "    elif model_type == 'gcForest':\n",
    "        feature_dim = train_features.shape[1]\n",
    "        param_grid = {\n",
    "            'window': [[int(feature_dim * 0.2)], [int(feature_dim * 0.3)]], 'n_mgsRFtree': [30],\n",
    "            'n_cascadeRFtree': [101], 'n_cascadeRF': [2], 'cascade_layer': [15], 'tolerance': [0.005]\n",
    "        }\n",
    "        model_base = gcForest(shape_1X=train_features.shape[1], n_jobs=-1)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model type: {model_type}\")\n",
    "    cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model_base, param_grid=param_grid, cv=cv, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "    grid_search.fit(train_features, train_labels)\n",
    "    print(f\"Best parameters for {model_type}: {grid_search.best_params_}\")\n",
    "    model = grid_search.best_estimator_\n",
    "    print(\"Evaluating the best model on the test set...\")\n",
    "    predictions = model.predict(test_features)\n",
    "    probas = model.predict_proba(test_features)\n",
    "    print(f\"Accuracy: {accuracy_score(test_labels, predictions):.4f}\")\n",
    "    print(f\"F1-score: {f1_score(test_labels, predictions, average='weighted'):.4f}\")\n",
    "    print(f\"Precision: {precision_score(test_labels, predictions, average='weighted'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(test_labels, predictions, average='weighted'):.4f}\")\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(test_labels, probas, multi_class='ovr', average='weighted')\n",
    "        print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not compute ROC AUC Score: {e}\")\n",
    "    print(\"\n",
    "Confusion Matrix:\")\n",
    "    print(confusion_matrix(test_labels, predictions))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def explain_model(model, test_features, feature_names, output_path):\n",
    "    print(\"Calculating SHAP values...\")\n",
    "    try:\n",
    "        background_data = shap.sample(test_features, 100)\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, background_data)\n",
    "        shap_values = explainer.shap_values(test_features)\n",
    "        print(\"Generating SHAP summary plot...\")\n",
    "        plt.figure()\n",
    "        if isinstance(shap_values, list):\n",
    "            shap.summary_plot(shap_values[0], test_features, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "        else:\n",
    "            shap.summary_plot(shap_values, test_features, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "        plt.title(\"SHAP Feature Importance\")\n",
    "        plot_file = os.path.join(output_path, 'shap_summary_plot.png')\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "        print(f\"SHAP summary plot saved to {plot_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate SHAP plot: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def run_experiment(args):\n",
    "    print(f\"====================--- Starting Experiment: Model={args.model}, Features={args.feature_extractor} ---\")\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(args.data_path, max_records=args.max_records)\n",
    "    train_features, test_features = extract_features(X_train, X_test, method=args.feature_extractor)\n",
    "    model = train_and_evaluate(train_features, y_train, test_features, y_test, model_type=args.model)\n",
    "    if args.explain:\n",
    "        feature_names = [f'MFCC_{i}' for i in range(train_features.shape[1])] if args.feature_extractor == 'MFCC' else [f'DWT_{i}' for i in range(train_features.shape[1]) ]\n",
    "        explain_model(model, test_features, feature_names, args.output_path)\n",
    "    print(f\"--- Experiment Finished: Model={args.model} ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Experiments\n",
    "\n",
    "Now we can run the experiments for both model types. We use a small number of records (`max_records=4`) for a quick test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args_cascade = argparse.Namespace(\n",
    "    data_path=DATA_PATH,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    feature_extractor='MFCC',\n",
    "    model='CascadeForest',\n",
    "    explain=True,\n",
    "    max_records=4\n",
    ")\n",
    "run_experiment(args_cascade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args_gc = argparse.Namespace(\n",
    "    data_path=DATA_PATH,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    feature_extractor='DWT',\n",
    "    model='gcForest',\n",
    "    explain=True,\n",
    "    max_records=4\n",
    ")\n",
    "run_experiment(args_gc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If the cells above executed without errors, your environment is correctly set up and the self-contained experiment notebook is working. You can now adjust the parameters (e.g., `max_records`, `feature_extractor`, and the `param_grid` in the `train_and_evaluate` function) to run your full research experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepforest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
